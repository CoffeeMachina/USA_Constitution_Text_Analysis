{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e89b17-b8fd-46a9-9dbc-34107db9bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import regex as re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715d0f23-333d-4d00-8c51-4d5999a2229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7641\n"
     ]
    }
   ],
   "source": [
    "with open(\"constitution.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "corpus=re.sub(r\"\\n\",\" \",corpus)\n",
    "\n",
    "# stops = stopwords.words(\"english\")\n",
    "print(len(corpus.split(' ')))\n",
    "\n",
    "#word ~ 7505 words\n",
    "# python split() ~ 7641 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5000146-64ca-417d-8da1-7e7349d628de",
   "metadata": {},
   "source": [
    "# <font color = 'limegreen'> General cleaning & Remove Stops</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ea9a91-f1f1-4955-8843-52465a4a7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stops(doc,stops):\n",
    "    #regex scrubber:\n",
    "    final=[]\n",
    "    clean=re.sub(r\"AC\\/\\d{1,4}\\/\\d{1,4}\", \"\",doc)\n",
    "    # clean=clean.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    for token in clean.split():\n",
    "        if token not in stops:\n",
    "            final.append(token)\n",
    "\n",
    "    final=\" \".join(final)\n",
    "    return final\n",
    "\n",
    "def clean_docs(docs):\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final=[]\n",
    "    for doc in docs:\n",
    "        final.append(clean_stops(doc,stops))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd45908-c9f2-44da-832d-05f1be5d70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_docs(corpus.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60514e5-04ea-4468-a64f-f5aaf90b60e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_docs(corpus.split(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2eb5c8-264d-4108-8329-3930935e7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = clean_docs(corpus.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9660f918-e98d-446f-b99e-e412bb767b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_joined = \" \".join(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae09ff3c-735d-4c50-a3dc-2dea9506ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean1=list(filter(None,clean))\n",
    "len(list(clean1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1151fe-4ca3-4820-a590-707dd22dc90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean) - len(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adeb8917-59a0-40ed-a389-f81e821b3e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4597839135654262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3447/len(clean)  #% of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede5ec2-4298-469d-97ba-cb936259face",
   "metadata": {},
   "source": [
    "# <font color = 'limegreen'> Tokenize Sentences: [IMP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb37c15-ddca-49d3-966c-1fdeb89928e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we', 'people', 'united', 'states', 'order', 'form', 'perfect', 'union', 'establish', 'justice', 'insure', 'domestic', 'tranquility', 'provide', 'common', 'defence', 'promote', 'general', 'welfare', 'secure', 'blessings', 'liberty', 'posterity', 'ordain', 'establish', 'constitution', 'united', 'states', 'america'], ['the', 'constitutional', 'convention', 'article', 'section', 'congress', 'all', 'legislative', 'powers', 'herein', 'granted', 'shall', 'vested', 'congress', 'united', 'states', 'shall', 'consist', 'senate', 'house', 'representatives']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(clean_sents))\n",
    "print(data_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2e8a6-24c7-4012-9c30-1de5d8ab4bb9",
   "metadata": {},
   "source": [
    "# <font color = 'limegreen'> Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f0a6eab-79a7-4983-8aa5-7bd679971d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','NER']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cba660-3174-4651-a248-c488172b5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words=lemmatization(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c3ee387-2f5f-4cee-ad8c-24e6d05037d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "\n",
    "\n",
    "# See trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words]])\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "# data_words_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_words)\n",
    "bi=[]\n",
    "tri=[]\n",
    "for layer1 in data_bigrams_trigrams:\n",
    "    for layer2 in layer1:\n",
    "        if layer2.count(\"_\") == 1 and layer2 not in bi:\n",
    "            bi.append(layer2)\n",
    "        elif layer2.count(\"_\") == 2 and layer2 not in tri:\n",
    "            tri.append(layer2)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "len(bi), len(tri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b924479-6b79-40e5-a7a3-c062ce644f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['appropriate_legislation'], ['article_appropriate_legislation'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi, tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1b56be-b027-4be4-9979-818064bde703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import TfidfModel\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:20])\n",
    "\n",
    "tfidf  = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "LOW_VALUE = 0.03\n",
    "words = []\n",
    "words_missing_in_tfidf=[]\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < LOW_VALUE]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe11179b-a6ef-4ef9-ad44-f63c2a92ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diablo\\anaconda3\\envs\\jpnlp\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\diablo\\anaconda3\\envs\\jpnlp\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\diablo\\anaconda3\\envs\\jpnlp\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\diablo\\anaconda3\\envs\\jpnlp\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyLDAvis\n",
    "\n",
    "HP_NUM_TOPICS = [5,10,15,20]\n",
    "HP_NUM_TERMS = [10,20,25,30]\n",
    "\n",
    "for topic, term in zip(HP_NUM_TOPICS,HP_NUM_TERMS):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=topic,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=\"auto\")\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus=corpus,dictionary= id2word, mds=\"mmds\", R=term)\n",
    "\n",
    "    pyLDAvis.save_html(vis,fr'LDA_Constitution_{topic}topics_{term}terms.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnlp100",
   "language": "python",
   "name": "jpnlp100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
